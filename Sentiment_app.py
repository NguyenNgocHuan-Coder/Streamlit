import streamlit as st
import pickle
import re, regex
import numpy as np
from underthesea import word_tokenize, pos_tag, sent_tokenize
from scipy.sparse import csr_matrix, hstack

# ========== Sidebar Menu ==========
st.sidebar.title("ğŸ“š Menu")
menu_choice = st.sidebar.radio("Chá»n chá»©c nÄƒng:", (
    "ğŸ“Œ Business Objective",
    "ğŸ—ï¸ Build Model",
    "ğŸ’¬ Sentiment Analysis",
    "ğŸ§© Information Clustering"
))
# ========== ThÃ´ng tin tÃ¡c giáº£ ==========
st.sidebar.markdown("""---""")
st.sidebar.markdown("""
**ğŸ“ TÃ¡c giáº£ Ä‘á»“ Ã¡n:**

- Nguyá»…n Ngá»c HuÃ¢n  
  âœ‰ï¸ *nguyenngochuan992@gmail.com*

- Nguyá»…n Thá»‹ Hoa Tháº¯ng  
  âœ‰ï¸ *thangnth0511@gmail.com*
""")
# ========== Load mÃ´ hÃ¬nh vÃ  vectorizer tá»« .pkl ==========
with open("vectorizer.pkl", "rb") as f:
    vectorizer = pickle.load(f)

with open("scaler.pkl", "rb") as f:
    scaler = pickle.load(f)

with open("model_lr.pkl", "rb") as f:
    model_lr = pickle.load(f)

with open("label_encoder.pkl", "rb") as f:
    le = pickle.load(f)

# ========== Load dictionary ==========
def load_dict_from_txt(path):
    with open(path, 'r', encoding='utf-8') as f:
        lines = f.read().splitlines()
    return dict(line.split('\t') for line in lines if '\t' in line)

def load_list_from_txt(path):
    with open(path, 'r', encoding='utf-8') as f:
        return [line.strip() for line in f if line.strip()]

emoji_dict = load_dict_from_txt("emojicon.txt")
teen_dict = load_dict_from_txt("teencode.txt")
wrong_lst = load_list_from_txt("wrong-word.txt")
stopwords_lst = load_list_from_txt("vietnamese-stopwords.txt")
positive_words = load_list_from_txt("positive_VN.txt")
negative_words = load_list_from_txt("negative_VN.txt")
positive_emojis = load_list_from_txt("positive_emoji.txt")
negative_emojis = load_list_from_txt("negative_emoji.txt")

# ========== Tiá»n xá»­ lÃ½ ==========
def covert_unicode(txt):
    return txt.encode('utf-8').decode('utf-8')

def normalize_repeated_characters(text):
    return re.sub(r'(.)\1+', r'\1', text)

def process_text(text):
    document = text.lower().replace("â€™", '')
    document = regex.sub(r'\.+', ".", document)
    new_sentence = ''
    for sentence in sent_tokenize(document):
        sentence = ''.join(emoji_dict.get(c, c) for c in sentence)
        sentence = ' '.join(teen_dict.get(w, w) for w in sentence.split())
        pattern = r'(?i)\b[a-zÃ¡Ã áº£Ã£áº¡Äƒáº¯áº±áº³áºµáº·Ã¢áº¥áº§áº©áº«áº­Ã©Ã¨áº»áº½áº¹Ãªáº¿á»á»ƒá»…á»‡Ã³Ã²á»Ãµá»Ã´á»‘á»“á»•á»—á»™Æ¡á»›á»á»Ÿá»¡á»£Ã­Ã¬á»‰Ä©á»‹ÃºÃ¹á»§Å©á»¥Æ°á»©á»«á»­á»¯á»±Ã½á»³á»·á»¹á»µÄ‘]+\b'
        sentence = ' '.join(regex.findall(pattern, sentence))
        sentence = ' '.join(w for w in sentence.split() if w not in wrong_lst)
        new_sentence += sentence + '. '
    return regex.sub(r'\s+', ' ', new_sentence).strip()

def process_postag_thesea(text):
    new_document = ''
    for sentence in sent_tokenize(text):
        sentence = sentence.replace('.', '')
        lst_word_type = ['N','Np','A','AB','V','VB','VY','R']
        sentence = ' '.join(
            word[0] if word[1] in lst_word_type else ''
            for word in pos_tag(word_tokenize(sentence, format="text"))
        )
        new_document += sentence + ' '
    return regex.sub(r'\s+', ' ', new_document).strip()

def remove_stopword(text):
    return regex.sub(r'\s+', ' ', ' '.join(w for w in text.split() if w not in stopwords_lst)).strip()

def count_sentiment_items(text):
    text = str(text).lower()
    pos_word = sum(1 for word in positive_words if word in text)
    pos_emoji = sum(text.count(emoji) for emoji in positive_emojis)
    neg_word = sum(1 for word in negative_words if word in text)
    neg_emoji = sum(text.count(emoji) for emoji in negative_emojis)
    return pos_word, neg_word, pos_emoji, neg_emoji

# ========== Dá»± Ä‘oÃ¡n ==========
def predict_sentiment(text_input, recommend_num):
    text = covert_unicode(text_input)
    text = normalize_repeated_characters(text)
    text = process_text(text)
    text = process_postag_thesea(text)
    text = remove_stopword(text)

    tfidf_vector = vectorizer.transform([text])
    pos_word, neg_word, pos_emoji, neg_emoji = count_sentiment_items(text_input)
    numeric_features = scaler.transform([[pos_word, neg_word, pos_emoji, neg_emoji]])
    recommend_feature = csr_matrix([[recommend_num]])

    final_features = hstack([tfidf_vector, csr_matrix(numeric_features), recommend_feature])
    y_pred = model_lr.predict(final_features)[0]
    label = le.inverse_transform([y_pred])[0]
    return label


# ========== CÃ¡c Trang á»¨ng Dá»¥ng ==========
if menu_choice == "ğŸ“Œ Business Objective":
    st.title("ğŸ“Œ Business Objective: Sentiment Analysis and Information Clustering")
    st.markdown("""
    #### Má»¥c tiÃªu cá»§a Ä‘á»“ Ã¡n:
    
    - **Sentiment Analysis**: XÃ¢y dá»±ng mÃ´ hÃ¬nh phÃ¢n loáº¡i cáº£m xÃºc tá»« cÃ¡c Ä‘Ã¡nh giÃ¡ cá»§a nhÃ¢n viÃªn/á»©ng viÃªn vá» cÃ´ng ty trÃªn ITviec (TÃ­ch cá»±c / Trung tÃ­nh / TiÃªu cá»±c). GiÃºp cÃ´ng ty náº¯m báº¯t Ä‘Æ°á»£c tÃ¢m lÃ½ ngÆ°á»i lao Ä‘á»™ng.

    - **Information Clustering**: PhÃ¢n cá»¥m cÃ¡c Ä‘Ã¡nh giÃ¡ Ä‘á»ƒ xÃ¡c Ä‘á»‹nh Ä‘áº·c Ä‘iá»ƒm ná»•i báº­t cá»§a tá»«ng nhÃ³m cÃ´ng ty, tá»« Ä‘Ã³ Ä‘á» xuáº¥t cÃ¡c cáº£i tiáº¿n Ä‘á»ƒ giá»¯ chÃ¢n nhÃ¢n viÃªn vÃ  nÃ¢ng cao tráº£i nghiá»‡m á»©ng viÃªn.

    #### á»¨ng dá»¥ng:
    
    - Há»‡ thá»‘ng Ä‘Ã¡nh giÃ¡ ná»™i bá»™ cho cÃ¡c cÃ´ng ty
    - CÃ´ng cá»¥ gá»£i Ã½ cáº£i thiá»‡n mÃ´i trÆ°á»ng lÃ m viá»‡c
    - Tá»± Ä‘á»™ng phÃ¢n tÃ­ch hÃ ng loáº¡t Ä‘Ã¡nh giÃ¡ tá»« ná»n táº£ng tuyá»ƒn dá»¥ng
    """)

elif menu_choice == "ğŸ—ï¸ Build Model":
    st.title("ğŸ—ï¸ Build Model")
    st.write("### Sentiment Analysis")
    st.write("##### 1. Data EDA")
    st.image("Sentiment_EDA.JPG")
    st.image("Clustering_EDA.JPG")
    st.write("##### 2. Visualization")
    st.image("sentiment_distributed_data.JPG")
    st.write("##### 3. Build model and Evaluation")
    st.write("###### - Huáº¥n luyá»‡n mÃ´ hÃ¬nh phÃ¢n loáº¡i Logistic Regression , Random Forest , Decision Tree")
    st.write("###### - ÄÃ¡nh giÃ¡ káº¿t quáº£ dá»±a trÃªn Presicion , ReCall , F1-Score , Accuracy")
    st.image("sentiment_evaluation.JPG")
    st.write("###### Confusion Matrix")
    st.image("Confusion Matrix.JPG")
    st.markdown("Chá»n mÃ´ hÃ¬nh <span style='color: red; font-weight: bold; text-decoration: underline'>Logistic Regression</span> lÃ  tá»‘i Æ°u nháº¥t.",
    unsafe_allow_html=True)
    st.write("### Information Clustering")
    st.write("##### 1. Data EDA")
    st.image("Clustering_EDA.JPG")
    st.write("##### 2. Visualization")
    st.image("Cluster_wordcloud.JPG")
    st.write("##### 3. Build model and Evaluation")
    st.write("###### - Huáº¥n luyá»‡n mÃ´ hÃ¬nh phÃ¢n cá»¥m vá»›i cÃ¡c thuáº­t toÃ¡n KMeans, AgglomerativeClustering, SpectralClustering, Birch")
    st.write("###### - ÄÃ¡nh giÃ¡ káº¿t quáº£ dá»±a trÃªn Sihouette score")
    st.image("k_evaluation.JPG")
    st.write("###### Trá»±c quan hoÃ¡ Elbow theo Sihouette score")
    st.image("ellbow.JPG")
    st.write("###### Trá»±c quan hoÃ¡ Elbow theo Sihouette score")
    st.image("Cluster_distributed.JPG")
    st.markdown(" Káº¿t luáº­n : Chá»n mÃ´ hÃ¬nh <span style='color: red; font-weight: bold; text-decoration: underline'>KMeans</span> vá»›i k=4 lÃ  mÃ´ hÃ¬nh tá»‘i Æ°u nháº¥t vÃ¬:",unsafe_allow_html=True)
    st.markdown(""" 
    - Silhouette Score â‰ˆ 0.75 cao nháº¥t vá»›i k=4, ráº¥t á»•n Ä‘á»‹nh.
    - CÃ¡c Ä‘iá»ƒm cÃ²n láº¡i giáº£m nháº¹ nhÆ°ng váº«n khÃ¡ cao â†’ á»•n Ä‘á»‹nh tá»‘t.
    - Biá»ƒu Ä‘á»“ phÃ¢n cá»¥m (LDA + KMeans): NhÃ³m dá»¯ liá»‡u Ä‘Æ°á»£c chia rÃµ rÃ ng, trá»±c quan.
    - Ranh giá»›i giá»¯a cÃ¡c cá»¥m rÃµ rÃ ng, gáº§n nhÆ° khÃ´ng cÃ³ Ä‘iá»ƒm chá»“ng láº¥n.
    """)
                
    st.write("##### 4. Interpreting and Visualizing Cluster Analysis Results")
    st.write("###### âœ… Chá»§ Ä‘á» #1:Báº¥t cáº­p trong Ä‘Ã£i ngá»™ & Ä‘iá»u kiá»‡n lÃ m viá»‡c. Cá»¥m nÃ y nháº¥n máº¡nh vá» cÃ¡c yáº¿u tá»‘ vá» lÆ°Æ¡ng vÃ  phÃºc lá»£i , Ä‘áº·c  biá»‡t cÃ³ Ä‘á» cáº­p Ä‘áº¿n váº¥n Ä‘á» báº¥t cáº­p lÃ  lÆ°Æ¡ng_cháº­m vÃ  cÃ´ng nghá»‡ cÅ©.")
    st.write("###### ğŸ”‘ Key words: chÃ­nh_sÃ¡ch_lÃ m_thÃªm_giá», cháº¿_Ä‘á»™_Ä‘Ã£i_ngá»™, cháº¿_Ä‘á»™_phÃºc_lá»£i, giá»_giáº¥c_thoáº£i_mÃ¡i, lÆ°Æ¡ng_cháº­m, lÆ°Æ¡ng_thÆ°á»Ÿng, sá»©c_khoáº», vÄƒn_phÃ²ng_Ä‘áº¹p, cÃ´ng_ty_lá»›n, Ä‘á»“ng_nghiá»‡p_thÃ¢n_thiá»‡n,mÃ´i_trÆ°á»ng_lÃ m_viá»‡c_thÃ¢n_thiá»‡n.")
    st.image("wordcloud_0.JPG")
    st.write("######  âœ… Chá»§ Ä‘á» #2: MÃ´i trÆ°á»ng & vÄƒn hÃ³a doanh nghiá»‡p .Táº­p trung vÃ o mÃ´i trÆ°á»ng lÃ m viá»‡c, vÄƒn hÃ³a cÃ´ng ty, vÃ  cÆ¡ sá»Ÿ váº­t cháº¥t há»— trá»£ nhÃ¢n viÃªn, Ä‘i kÃ¨m má»™t sá»‘ yáº¿u tá»‘ vá» chÃ­nh sÃ¡ch vÃ  lÆ°Æ¡ng")
    st.write("###### ğŸ”‘ Key words: mÃ´i_trÆ°á»ng_lÃ m_viá»‡c_tá»‘t, mÃ´i_trÆ°á»ng_lÃ m_viá»‡c_thoáº£i_mÃ¡i, vÄƒn_hoÃ¡_cÃ´ng_ty, mÃ´i_trÆ°á»ng_lÃ m_viá»‡c_nÄƒng_Ä‘á»™ng, vÄƒn_hoÃ¡_cÃ´ng_ty, Ä‘á»“ng_nghiá»‡p_thÃ¢n_thiá»‡n, cÃ´ng_ty_lá»›n, vÄƒn_phÃ²ng_Ä‘áº¹p ,bÃ£i_Ä‘áº­u_xe_rá»™ng_rÃ£i, lÆ°Æ¡ng_thÆ°á»Ÿng, chÃ­nh_sÃ¡ch_lÃ m_thÃªm_giá».")
    st.image("wordcloud_1.JPG")
    st.write("######  âœ… Chá»§ Ä‘á» #3: ÄÃ£i ngá»™ & cÆ¡ há»™i phÃ¡t triá»ƒn . Gáº§n giá»‘ng cá»¥m 0 nhÆ°ng nháº¥n máº¡nh thÃªm vÃ o yáº¿u tá»‘ phÃºc lá»£i, dá»± Ã¡n lá»›n vÃ  má»©c lÆ°Æ¡ng tá»‘t â†’ thá»ƒ hiá»‡n sá»± quan tÃ¢m Ä‘áº¿n giÃ¡ trá»‹ cÃ´ng viá»‡c & Ä‘Ã£i ngá»™.")
    st.write("###### ğŸ”‘ Key words: dá»±_Ã¡n_lá»›n, lÆ°Æ¡ng_tá»‘t, lÆ°Æ¡ng_á»•n,mÃ´i_trÆ°á»ng_lÃ m_viá»‡c_thoáº£i_mÃ¡i, mÃ´i_trÆ°á»ng_lÃ m_viá»‡c_thÃ¢n_thiá»‡n, cháº¿_Ä‘á»™_phÃºc_lá»£i, Ä‘á»“ng_nghiá»‡p_thÃ¢n_thiá»‡n, vÄƒn_phÃ²ng_rá»™ng")
    st.image("wordcloud_2.JPG")
    st.write("######  âœ… Chá»§ Ä‘á» #4: Tráº£i nghiá»‡m lÃ m viá»‡c tÃ­ch cá»±c . Cá»¥m nÃ y thá»ƒ hiá»‡n rÃµ yáº¿u tá»‘ tráº£i nghiá»‡m lÃ m viá»‡c hÃ ng ngÃ y: linh hoáº¡t, vÄƒn phÃ²ng Ä‘áº¹p, Ä‘á»“ng nghiá»‡p vui váº», vÄƒn hÃ³a tÃ­ch cá»±c.")
    st.write("###### ğŸ”‘ Key words: vÄƒn_phÃ²ng_Ä‘áº¹p, vÄƒn_phÃ²ng_rá»™ng_rÃ£i, phong_cáº£nh_Ä‘áº¹p, chÃ­nh_sÃ¡ch_lÃ m_thÃªm_giá», lÆ°Æ¡ng_thÆ°á»Ÿng, Ä‘á»“ng_nghiá»‡p_thÃ¢n_thiá»‡n, mÃ´i_trÆ°á»ng_lÃ m_viá»‡c_tá»‘t, cÃ´ng_ty_lá»›n, bÃ£i_Ä‘áº­u_xe_rá»™ng_rÃ£i, mÃ´i_trÆ°á»ng_lÃ m_viá»‡c_nÄƒng_Ä‘á»™ng, mÃ´i_trÆ°á»ng_lÃ m_viá»‡c_thoáº£i_mÃ¡i, vÄƒn_hÃ³a_cÃ´ng_ty.")
    st.image("wordcloud_3.JPG")
elif menu_choice == "ğŸ’¬ Sentiment Analysis":
    st.title("ğŸ’¬ á»¨ng dá»¥ng phÃ¢n tÃ­ch cáº£m xÃºc review cÃ´ng ty")

    input_text = st.text_area("âœï¸ Nháº­p cÃ¢u Ä‘Ã¡nh giÃ¡ cá»§a báº¡n:", height=150)
    recommend_input = st.checkbox("âœ… Báº¡n cÃ³ recommend cÃ´ng ty nÃ y khÃ´ng?", value=True)
    recommend_num = 1 if recommend_input else 0

    if st.button("ğŸš€ Dá»± Ä‘oÃ¡n cáº£m xÃºc"):
        if not input_text.strip():
            st.warning("â›” Vui lÃ²ng nháº­p ná»™i dung review!")
        else:
            with st.spinner("ğŸ” Äang xá»­ lÃ½..."):
                result = predict_sentiment(input_text, recommend_num)
            st.success(f"âœ… Káº¿t quáº£ dá»± Ä‘oÃ¡n: **{result.upper()}**")

elif menu_choice == "ğŸ§© Information Clustering":
    st.title("ğŸ§© Information Clustering")
    st.info("ğŸ› ï¸ PhÃ¢n cá»¥m Ä‘Ã¡nh giÃ¡ cÃ´ng ty sáº½ Ä‘Æ°á»£c cáº­p nháº­t sau")


