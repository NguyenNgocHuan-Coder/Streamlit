import streamlit as st
import pickle
import re, regex
import numpy as np
from underthesea import word_tokenize, pos_tag, sent_tokenize
from scipy.sparse import csr_matrix, hstack
import pandas as pd
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
# ========== Sidebar Menu ==========
st.sidebar.title("üìö Menu")
menu_choice = st.sidebar.radio("Ch·ªçn ch·ª©c nƒÉng:", (
    "üìå Business Objective",
    "üèóÔ∏è Build Model",
    "üí¨ Sentiment Analysis",
    "üß© Information Clustering"
))
# ========== Th√¥ng tin t√°c gi·∫£ ==========
st.sidebar.markdown("""---""")
st.sidebar.markdown("""
**üéì T√°c gi·∫£ ƒë·ªì √°n:**

- Nguy·ªÖn Ng·ªçc Hu√¢n  
  ‚úâÔ∏è *nguyenngochuan992@gmail.com*

- Nguy·ªÖn Th·ªã Hoa Th·∫Øng  
  ‚úâÔ∏è *thangnth0511@gmail.com*
""")
# ========== Load m√¥ h√¨nh v√† vectorizer t·ª´ .pkl ==========
with open("vectorizer.pkl", "rb") as f:
    vectorizer = pickle.load(f)

with open("scaler.pkl", "rb") as f:
    scaler = pickle.load(f)

with open("model_lr.pkl", "rb") as f:
    model_lr = pickle.load(f)

with open("label_encoder.pkl", "rb") as f:
    le = pickle.load(f)

# ========== Load dictionary ==========
def load_dict_from_txt(path):
    with open(path, 'r', encoding='utf-8') as f:
        lines = f.read().splitlines()
    return dict(line.split('\t') for line in lines if '\t' in line)

def load_list_from_txt(path):
    with open(path, 'r', encoding='utf-8') as f:
        return [line.strip() for line in f if line.strip()]

emoji_dict = load_dict_from_txt("emojicon.txt")
teen_dict = load_dict_from_txt("teencode.txt")
wrong_lst = load_list_from_txt("wrong-word.txt")
stopwords_lst = load_list_from_txt("vietnamese-stopwords.txt")
positive_words = load_list_from_txt("positive_VN.txt")
negative_words = load_list_from_txt("negative_VN.txt")
positive_emojis = load_list_from_txt("positive_emoji.txt")
negative_emojis = load_list_from_txt("negative_emoji.txt")
correct_dict = load_list_from_txt("phrase_corrections.txt")
english_dict = load_list_from_txt("english-vnmese.txt")
# ========== Ti·ªÅn x·ª≠ l√Ω ==========
def covert_unicode(txt):
    return txt.encode('utf-8').decode('utf-8')

def normalize_repeated_characters(text):
    return re.sub(r'(.)\1+', r'\1', text)

def process_text(text):
    document = text.lower().replace("‚Äô", '')
    document = regex.sub(r'\.+', ".", document)
    new_sentence = ''
    for sentence in sent_tokenize(document):
        sentence = ''.join(emoji_dict.get(c, c) for c in sentence)
        sentence = ' '.join(teen_dict.get(w, w) for w in sentence.split())
        pattern = r'(?i)\b[a-z√°√†·∫£√£·∫°ƒÉ·∫Ø·∫±·∫≥·∫µ·∫∑√¢·∫•·∫ß·∫©·∫´·∫≠√©√®·∫ª·∫Ω·∫π√™·∫ø·ªÅ·ªÉ·ªÖ·ªá√≥√≤·ªè√µ·ªç√¥·ªë·ªì·ªï·ªó·ªô∆°·ªõ·ªù·ªü·ª°·ª£√≠√¨·ªâƒ©·ªã√∫√π·ªß≈©·ª•∆∞·ª©·ª´·ª≠·ªØ·ª±√Ω·ª≥·ª∑·ªπ·ªµƒë]+\b'
        sentence = ' '.join(regex.findall(pattern, sentence))
        sentence = ' '.join(w for w in sentence.split() if w not in wrong_lst)
        new_sentence += sentence + '. '
    return regex.sub(r'\s+', ' ', new_sentence).strip()

def process_postag_thesea(text):
    new_document = ''
    for sentence in sent_tokenize(text):
        sentence = sentence.replace('.', '')
        lst_word_type = ['N','Np','A','AB','V','VB','VY','R']
        sentence = ' '.join(
            word[0] if word[1] in lst_word_type else ''
            for word in pos_tag(word_tokenize(sentence, format="text"))
        )
        new_document += sentence + ' '
    return regex.sub(r'\s+', ' ', new_document).strip()

def remove_stopword(text):
    return regex.sub(r'\s+', ' ', ' '.join(w for w in text.split() if w not in stopwords_lst)).strip()

def count_sentiment_items(text):
    text = str(text).lower()
    pos_word = sum(1 for word in positive_words if word in text)
    pos_emoji = sum(text.count(emoji) for emoji in positive_emojis)
    neg_word = sum(1 for word in negative_words if word in text)
    neg_emoji = sum(text.count(emoji) for emoji in negative_emojis)
    return pos_word, neg_word, pos_emoji, neg_emoji

# ========== D·ª± ƒëo√°n ==========
def predict_sentiment(text_input, recommend_num):
    text = covert_unicode(text_input)
    text = normalize_repeated_characters(text)
    text = process_text(text)
    text = process_postag_thesea(text)
    text = remove_stopword(text)

    tfidf_vector = vectorizer.transform([text])
    pos_word, neg_word, pos_emoji, neg_emoji = count_sentiment_items(text_input)
    numeric_features = scaler.transform([[pos_word, neg_word, pos_emoji, neg_emoji]])
    recommend_feature = csr_matrix([[recommend_num]])

    final_features = hstack([tfidf_vector, csr_matrix(numeric_features), recommend_feature])
    y_pred = model_lr.predict(final_features)[0]
    label = le.inverse_transform([y_pred])[0]
    return label


# ========== C√°c Trang ·ª®ng D·ª•ng ==========
if menu_choice == "üìå Business Objective":
    st.title("üìå Business Objective: Sentiment Analysis and Information Clustering")
    st.markdown("""
    #### M·ª•c ti√™u c·ªßa ƒë·ªì √°n:
    
    - **Sentiment Analysis**: X√¢y d·ª±ng m√¥ h√¨nh ph√¢n lo·∫°i c·∫£m x√∫c t·ª´ c√°c ƒë√°nh gi√° c·ªßa nh√¢n vi√™n/·ª©ng vi√™n v·ªÅ c√¥ng ty tr√™n ITviec (T√≠ch c·ª±c / Trung t√≠nh / Ti√™u c·ª±c). Gi√∫p c√¥ng ty n·∫Øm b·∫Øt ƒë∆∞·ª£c t√¢m l√Ω ng∆∞·ªùi lao ƒë·ªông.

    - **Information Clustering**: Ph√¢n c·ª•m c√°c ƒë√°nh gi√° ƒë·ªÉ x√°c ƒë·ªãnh ƒë·∫∑c ƒëi·ªÉm n·ªïi b·∫≠t c·ªßa t·ª´ng nh√≥m c√¥ng ty, t·ª´ ƒë√≥ ƒë·ªÅ xu·∫•t c√°c c·∫£i ti·∫øn ƒë·ªÉ gi·ªØ ch√¢n nh√¢n vi√™n v√† n√¢ng cao tr·∫£i nghi·ªám ·ª©ng vi√™n.

    #### ·ª®ng d·ª•ng:
    
    - H·ªá th·ªëng ƒë√°nh gi√° n·ªôi b·ªô cho c√°c c√¥ng ty
    - C√¥ng c·ª• g·ª£i √Ω c·∫£i thi·ªán m√¥i tr∆∞·ªùng l√†m vi·ªác
    - T·ª± ƒë·ªông ph√¢n t√≠ch h√†ng lo·∫°t ƒë√°nh gi√° t·ª´ n·ªÅn t·∫£ng tuy·ªÉn d·ª•ng
    """)

elif menu_choice == "üèóÔ∏è Build Model":
    st.title("üèóÔ∏è Build Model")
    st.write("### Sentiment Analysis")
    st.write("##### 1. Data EDA")
    st.image("Sentiment_EDA.JPG")
    st.image("Clustering_EDA.JPG")
    st.write("##### 2. Visualization")
    st.image("sentiment_distributed_data.JPG")
    st.write("##### 3. Build model and Evaluation")
    st.write("###### - Hu·∫•n luy·ªán m√¥ h√¨nh ph√¢n lo·∫°i Logistic Regression , Random Forest , Decision Tree")
    st.write("###### - ƒê√°nh gi√° k·∫øt qu·∫£ d·ª±a tr√™n Presicion , ReCall , F1-Score , Accuracy")
    st.image("sentiment_evaluation.JPG")
    st.write("###### Confusion Matrix")
    st.image("Confusion Matrix.JPG")
    st.markdown("Ch·ªçn m√¥ h√¨nh <span style='color: red; font-weight: bold; text-decoration: underline'>Logistic Regression</span> l√† t·ªëi ∆∞u nh·∫•t.",
    unsafe_allow_html=True)
    st.write("### Information Clustering")
    st.write("##### 1. Data EDA")
    st.image("Clustering_EDA.JPG")
    st.write("##### 2. Visualization")
    st.image("Cluster_wordcloud.JPG")
    st.write("##### 3. Build model and Evaluation")
    st.write("###### - Hu·∫•n luy·ªán m√¥ h√¨nh ph√¢n c·ª•m v·ªõi c√°c thu·∫≠t to√°n KMeans, AgglomerativeClustering, SpectralClustering, Birch")
    st.write("###### - ƒê√°nh gi√° k·∫øt qu·∫£ d·ª±a tr√™n Sihouette score")
    st.image("k_evaluation.JPG")
    st.write("###### Tr·ª±c quan ho√° Elbow theo Sihouette score")
    st.image("ellbow.JPG")
    st.write("###### Tr·ª±c quan ho√° Elbow theo Sihouette score")
    st.image("Cluster_distributed.JPG")
    st.markdown(" K·∫øt lu·∫≠n : Ch·ªçn m√¥ h√¨nh <span style='color: red; font-weight: bold; text-decoration: underline'>KMeans</span> v·ªõi k=4 l√† m√¥ h√¨nh t·ªëi ∆∞u nh·∫•t v√¨:",unsafe_allow_html=True)
    st.markdown(""" 
    - Silhouette Score ‚âà 0.75 cao nh·∫•t v·ªõi k=4, r·∫•t ·ªïn ƒë·ªãnh.
    - C√°c ƒëi·ªÉm c√≤n l·∫°i gi·∫£m nh·∫π nh∆∞ng v·∫´n kh√° cao ‚Üí ·ªïn ƒë·ªãnh t·ªët.
    - Bi·ªÉu ƒë·ªì ph√¢n c·ª•m (LDA + KMeans): Nh√≥m d·ªØ li·ªáu ƒë∆∞·ª£c chia r√µ r√†ng, tr·ª±c quan.
    - Ranh gi·ªõi gi·ªØa c√°c c·ª•m r√µ r√†ng, g·∫ßn nh∆∞ kh√¥ng c√≥ ƒëi·ªÉm ch·ªìng l·∫•n.
    """)
                
    st.write("##### 4. Interpreting and Visualizing Cluster Analysis Results")
    st.write("###### ‚úÖ Ch·ªß ƒë·ªÅ #1:B·∫•t c·∫≠p trong ƒë√£i ng·ªô & ƒëi·ªÅu ki·ªán l√†m vi·ªác. C·ª•m n√†y nh·∫•n m·∫°nh v·ªÅ c√°c y·∫øu t·ªë v·ªÅ l∆∞∆°ng v√† ph√∫c l·ª£i , ƒë·∫∑c  bi·ªát c√≥ ƒë·ªÅ c·∫≠p ƒë·∫øn v·∫•n ƒë·ªÅ b·∫•t c·∫≠p l√† l∆∞∆°ng_ch·∫≠m v√† c√¥ng ngh·ªá c≈©.")
    st.write("###### üîë Key words: ch√≠nh_s√°ch_l√†m_th√™m_gi·ªù, ch·∫ø_ƒë·ªô_ƒë√£i_ng·ªô, ch·∫ø_ƒë·ªô_ph√∫c_l·ª£i, gi·ªù_gi·∫•c_tho·∫£i_m√°i, l∆∞∆°ng_ch·∫≠m, l∆∞∆°ng_th∆∞·ªüng, s·ª©c_kho·∫ª, vƒÉn_ph√≤ng_ƒë·∫πp, c√¥ng_ty_l·ªõn, ƒë·ªìng_nghi·ªáp_th√¢n_thi·ªán,m√¥i_tr∆∞·ªùng_l√†m_vi·ªác_th√¢n_thi·ªán.")
    st.image("wordcloud_0.JPG")
    st.write("######  ‚úÖ Ch·ªß ƒë·ªÅ #2: M√¥i tr∆∞·ªùng & vƒÉn h√≥a doanh nghi·ªáp .T·∫≠p trung v√†o m√¥i tr∆∞·ªùng l√†m vi·ªác, vƒÉn h√≥a c√¥ng ty, v√† c∆° s·ªü v·∫≠t ch·∫•t h·ªó tr·ª£ nh√¢n vi√™n, ƒëi k√®m m·ªôt s·ªë y·∫øu t·ªë v·ªÅ ch√≠nh s√°ch v√† l∆∞∆°ng")
    st.write("###### üîë Key words: m√¥i_tr∆∞·ªùng_l√†m_vi·ªác_t·ªët, m√¥i_tr∆∞·ªùng_l√†m_vi·ªác_tho·∫£i_m√°i, vƒÉn_ho√°_c√¥ng_ty, m√¥i_tr∆∞·ªùng_l√†m_vi·ªác_nƒÉng_ƒë·ªông, vƒÉn_ho√°_c√¥ng_ty, ƒë·ªìng_nghi·ªáp_th√¢n_thi·ªán, c√¥ng_ty_l·ªõn, vƒÉn_ph√≤ng_ƒë·∫πp ,b√£i_ƒë·∫≠u_xe_r·ªông_r√£i, l∆∞∆°ng_th∆∞·ªüng, ch√≠nh_s√°ch_l√†m_th√™m_gi·ªù.")
    st.image("wordcloud_1.JPG")
    st.write("######  ‚úÖ Ch·ªß ƒë·ªÅ #3: ƒê√£i ng·ªô & c∆° h·ªôi ph√°t tri·ªÉn . G·∫ßn gi·ªëng c·ª•m 0 nh∆∞ng nh·∫•n m·∫°nh th√™m v√†o y·∫øu t·ªë ph√∫c l·ª£i, d·ª± √°n l·ªõn v√† m·ª©c l∆∞∆°ng t·ªët ‚Üí th·ªÉ hi·ªán s·ª± quan t√¢m ƒë·∫øn gi√° tr·ªã c√¥ng vi·ªác & ƒë√£i ng·ªô.")
    st.write("###### üîë Key words: d·ª±_√°n_l·ªõn, l∆∞∆°ng_t·ªët, l∆∞∆°ng_·ªïn,m√¥i_tr∆∞·ªùng_l√†m_vi·ªác_tho·∫£i_m√°i, m√¥i_tr∆∞·ªùng_l√†m_vi·ªác_th√¢n_thi·ªán, ch·∫ø_ƒë·ªô_ph√∫c_l·ª£i, ƒë·ªìng_nghi·ªáp_th√¢n_thi·ªán, vƒÉn_ph√≤ng_r·ªông")
    st.image("wordcloud_2.JPG")
    st.write("######  ‚úÖ Ch·ªß ƒë·ªÅ #4: Tr·∫£i nghi·ªám l√†m vi·ªác t√≠ch c·ª±c . C·ª•m n√†y th·ªÉ hi·ªán r√µ y·∫øu t·ªë tr·∫£i nghi·ªám l√†m vi·ªác h√†ng ng√†y: linh ho·∫°t, vƒÉn ph√≤ng ƒë·∫πp, ƒë·ªìng nghi·ªáp vui v·∫ª, vƒÉn h√≥a t√≠ch c·ª±c.")
    st.write("###### üîë Key words: vƒÉn_ph√≤ng_ƒë·∫πp, vƒÉn_ph√≤ng_r·ªông_r√£i, phong_c·∫£nh_ƒë·∫πp, ch√≠nh_s√°ch_l√†m_th√™m_gi·ªù, l∆∞∆°ng_th∆∞·ªüng, ƒë·ªìng_nghi·ªáp_th√¢n_thi·ªán, m√¥i_tr∆∞·ªùng_l√†m_vi·ªác_t·ªët, c√¥ng_ty_l·ªõn, b√£i_ƒë·∫≠u_xe_r·ªông_r√£i, m√¥i_tr∆∞·ªùng_l√†m_vi·ªác_nƒÉng_ƒë·ªông, m√¥i_tr∆∞·ªùng_l√†m_vi·ªác_tho·∫£i_m√°i, vƒÉn_h√≥a_c√¥ng_ty.")
    st.image("wordcloud_3.JPG")
elif menu_choice == "üí¨ Sentiment Analysis":
    st.title("üí¨ ·ª®ng d·ª•ng ph√¢n t√≠ch c·∫£m x√∫c review c√¥ng ty")

    input_text = st.text_area("‚úçÔ∏è Nh·∫≠p c√¢u ƒë√°nh gi√° c·ªßa b·∫°n:", height=150)
    recommend_input = st.checkbox("‚úÖ B·∫°n c√≥ recommend c√¥ng ty n√†y kh√¥ng?", value=True)
    recommend_num = 1 if recommend_input else 0

    if st.button("üöÄ D·ª± ƒëo√°n c·∫£m x√∫c"):
        if not input_text.strip():
            st.warning("‚õî Vui l√≤ng nh·∫≠p n·ªôi dung review!")
        else:
            with st.spinner("üîç ƒêang x·ª≠ l√Ω..."):
                result = predict_sentiment(input_text, recommend_num)
            st.success(f"‚úÖ K·∫øt qu·∫£ d·ª± ƒëo√°n: **{result.upper()}**")

elif menu_choice == "üß© Information Clustering":
    st.title("üß© Information Clustering")
    
    try:
        #LOAD EMOJICON
        file = open('emojicon.txt', 'r', encoding="utf8")
        emoji_lst = file.read().split('\n')
        emoji_dict1 = {}
        for line in emoji_lst:
            key, value = line.split('\t')
            emoji_dict1[key] = str(value)
        file.close()
        #################
        #LOAD TEENCODE
        file = open('teencode.txt', 'r', encoding="utf8")
        teen_lst = file.read().split('\n')
        teen_dict1 = {}
        for line in teen_lst:
            key, value = line.split('\t')
            teen_dict1[key] = str(value)
        file.close()

        ###############
        #LOAD TRANSLATE ENGLISH -> VNMESE
        file = open('english-vnmese.txt', 'r', encoding="utf8")
        english_lst = file.read().split('\n')
        english_dict1 = {}
        for line in english_lst:
            parts = line.strip().split('\t')
            if len(parts) >= 2:
                key = parts[0]
                value = '\t'.join(parts[1:])  # n·∫øu value c√≥ ch·ª©a d·∫•u tab th√¨ v·∫´n gi·ªØ nguy√™n
            english_dict1[key] = value
        file.close()

        ################
        #LOAD wrong words
        file = open('wrong-word.txt', 'r', encoding="utf8")
        wrong_lst1 = file.read().split('\n')
        file.close()

        #################
        #LOAD STOPWORDS
        file = open('vietnamese-stopwords.txt', 'r', encoding="utf8")
        stopwords_lst1 = file.read().split('\n')
        file.close()

        #################
        ##LOAD PHRASE_CORRECTION
        file = open('phrase_corrections.txt', 'r', encoding="utf8")
        correct_lst = file.read().split('\n')
        correct_dict1 = {}
        for line in correct_lst:
            key, value = line.split(':')
            correct_dict1[key] = str(value)
        file.close()
        df = pd.read_excel("Reviews.xlsx", engine="openpyxl")
        df["Review"] = df["What I liked"].fillna("") + " " + df["Suggestions for improvement"].fillna("")
        df = df[["Company Name", "Review"]].dropna()

        # Select box ch·ªçn c√¥ng ty
        company_list_all = sorted(df["Company Name"].dropna().unique())
        selected_company = st.selectbox("üîé Ch·ªçn c√¥ng ty ƒë·ªÉ ph√¢n t√≠ch:", company_list_all)

        df = df[df["Company Name"] == selected_company]
        def apply_phrase_correction(sentence, correct_dict1):
            for phrase, corrected in correct_dict1.items():
                # D√πng regex ƒë·ªÉ thay th·∫ø c·ª•m t·ª´ ch√≠nh x√°c (c√≥ ph√¢n c√°ch b·∫±ng d·∫•u c√°ch)
                pattern = r'\b' + regex.escape(phrase) + r'\b'
                sentence = regex.sub(pattern, corrected, sentence)
            return sentence        
        def process_text(text, emoji_dict1, teen_dict1, english_dict1, correct_dict1, wrong_lst1,stopwords_lst1):
            #Chuy·ªÉn vƒÉn b·∫£n th√†nh ch·ªØ th∆∞·ªùng
            document = text.lower()
            document = document.replace("‚Äô",'')
            document = regex.sub(r'\.+', ".", document)
            new_sentence = ''
            for sentence in sent_tokenize(document):
                #CONVERT EMOJICON
                sentence = ''.join(emoji_dict1[word] + ' ' if word in emoji_dict1 else word for word in list(sentence))

                #CONVERT TEENCODE
                sentence = ' '.join(teen_dict1[word] if word in teen_dict1 else word for word in sentence.split())

                #CONVERT ENGLISH TO VIETNAMESE
                sentence = ' '.join(english_dict1[word] if word in english_dict1 else word for word in sentence.split())

                #DEL Punctuation & Numbers (ch·ªâ gi·ªØ t·ª´ ti·∫øng Vi·ªát, k·ªÉ c·∫£ c√≥ d·∫•u)
                pattern = r'(?i)\b[a-z√°√†·∫£√£·∫°ƒÉ·∫Ø·∫±·∫≥·∫µ·∫∑√¢·∫•·∫ß·∫©·∫´·∫≠√©√®·∫ª·∫Ω·∫π√™·∫ø·ªÅ·ªÉ·ªÖ·ªá√≥√≤·ªè√µ·ªç√¥·ªë·ªì·ªï·ªó·ªô∆°·ªõ·ªù·ªü·ª°·ª£√≠√¨·ªâƒ©·ªã√∫√π·ªß≈©·ª•∆∞·ª©·ª´·ª≠·ªØ·ª±√Ω·ª≥·ª∑·ªπ·ªµƒë]+\b'
                sentence = ' '.join(regex.findall(pattern, sentence))

                #CONVERT PHRASE CORRECTION
                sentence = apply_phrase_correction(sentence, correct_dict1)

                #DEL wrong words
                # sentence = ' '.join(word for word in sentence.split() if word not in wrong_lst1)

                #DEL stop words
                # sentence = ' '.join(word for word in sentence.split() if word not in stopwords_lst1)

                new_sentence = new_sentence + sentence + '. '

            document = new_sentence
            ###### DEL excess blank space
            document = regex.sub(r'\s+', ' ', document).strip()

            return document

        # Ti·ªÅn x·ª≠ l√Ω vƒÉn b·∫£n
        df["Cleaned"] = df['Review'].apply(lambda text: process_text(text, emoji_dict1, teen_dict1, english_dict1, correct_dict1, wrong_lst1,stopwords_lst1))
        #T√°ch t·ª´
        def work_tokenize(text):
            tokens = word_tokenize(text, format='text')
            return tokens
        df["Cleaned"] = df["Cleaned"].apply(lambda text: work_tokenize(text)) 
        #N·ªëi t·ª´ ph·ªß ƒë·ªãnh v·ªõi t·ª´ li·ªÅn sau n√≥ :
        def merge_negation_words(text):
            pattern = r"\b(kh√¥ng|kh√¥ng_c√≥|ch∆∞a|ch∆∞a_c√≥|kh√≥|√≠t|√≠t_khi|hi·∫øm|thi·∫øu)\s+(\p{L}+)"
            return regex.sub(pattern, r"\1_\2", text)
        df["Cleaned"] = df["Cleaned"].apply(lambda text: merge_negation_words(text))
        def remove_stopwords_and_dedup(text):
            # T√°ch t·ª´
            words = text.split()

            # Lo·∫°i b·ªè stopwords
            filtered = [word for word in words if word not in stopwords_lst1]

            # Lo·∫°i b·ªè t·ª´/c·ª•m t·ª´ tr√πng nhau li·ªÅn k·ªÅ
            deduped = []
            prev_word = None
            for word in filtered:
                if word != prev_word:
                    deduped.append(word)
                prev_word = word

            return " ".join(deduped)   
        df["Cleaned"] = df["Cleaned"].apply(lambda text: remove_stopwords_and_dedup(text))
        def postag_merge(text):
            # G√°n nh√£n t·ª´ lo·∫°i
            tagged = pos_tag(text)

            # G·ªôp: danh t·ª´ + (t√≠nh t·ª´ | ƒë·ªông t·ª´), ho·∫∑c ƒë·ªông t·ª´ + t√≠nh t·ª´
            merged_words = []
            skip = False
            for i in range(len(tagged)):
                if skip:
                    skip = False
                    continue

                word, tag = tagged[i]

                if i + 1 < len(tagged):
                    next_word, next_tag = tagged[i + 1]

                    # N·ªëi danh t·ª´ v·ªõi t√≠nh t·ª´ ho·∫∑c ƒë·ªông t·ª´
                    if tag == 'N' and next_tag in {'A', 'V'}:
                        merged_words.append(f"{word}_{next_word}")
                        skip = True
                    # N·ªëi ƒë·ªông t·ª´ v·ªõi t√≠nh t·ª´
                    # elif tag == 'V' and next_tag == 'A':
                    #     merged_words.append(f"{word}_{next_word}")
                    #     skip = True
                    # else:
                    #     merged_words.append(word)
                else:
                    merged_words.append(word)

            return " ".join(merged_words)
        df["Cleaned"] = df["Cleaned"].apply(lambda text: postag_merge(text))
        df["Cleaned"] = df["Cleaned"].apply(lambda text: apply_phrase_correction(text, correct_dict1))    
        # Vector h√≥a vƒÉn b·∫£n
        vectorizer_cluster = CountVectorizer(max_df=0.95, min_df=20)
        X_vec = vectorizer_cluster.fit_transform(df["Cleaned"])

        # Ph√¢n c·ª•m v·ªõi KMeans
        kmeans = KMeans(n_clusters=4, random_state=42)
        df["Cluster"] = kmeans.fit_predict(X_vec)

        # T·ª´ kh√≥a ƒë·∫∑c tr∆∞ng theo c·ª•m
        keywords = vectorizer_cluster.get_feature_names_out()
        order_centroids = kmeans.cluster_centers_.argsort()[:, ::-1]
        cluster_keywords = [", ".join([keywords[i] for i in order_centroids[c][:10]]) for c in range(5)]
        df["Top Keywords"] = df["Cluster"].map({i: kw for i, kw in enumerate(cluster_keywords)})

        cluster_id = df["Cluster"].iloc[0]
        top_keywords = df["Top Keywords"].iloc[0]

        st.markdown(f"‚úÖ **C√¥ng ty thu·ªôc c·ª•m s·ªë:** `{cluster_id}`")
        st.markdown(f"üîë **T·ª´ kh√≥a ƒë·∫∑c tr∆∞ng c·ªßa c·ª•m:** {top_keywords}")
        st.markdown(f"üìù S·ªë l∆∞·ª£ng ƒë√°nh gi√°: {df.shape[0]}")

    except Exception as e:
        st.error(f"L·ªói ƒë·ªçc ho·∫∑c x·ª≠ l√Ω d·ªØ li·ªáu: {e}")
