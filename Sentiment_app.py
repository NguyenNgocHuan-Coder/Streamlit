import streamlit as st
import pickle
import re, regex
import numpy as np
from underthesea import word_tokenize, pos_tag, sent_tokenize
from scipy.sparse import csr_matrix, hstack

# ========== Sidebar Menu ==========
st.sidebar.title("ğŸ“š Menu")
menu_choice = st.sidebar.radio("Chá»n chá»©c nÄƒng:", (
    "ğŸ“Œ Business Objective",
    "ğŸ—ï¸ Build Model",
    "ğŸ’¬ Sentiment Analysis",
    "ğŸ§© Information Clustering"
))
# ========== Load mÃ´ hÃ¬nh vÃ  vectorizer tá»« .pkl ==========
with open("vectorizer.pkl", "rb") as f:
    vectorizer = pickle.load(f)

with open("scaler.pkl", "rb") as f:
    scaler = pickle.load(f)

with open("model_lr.pkl", "rb") as f:
    model_lr = pickle.load(f)

with open("label_encoder.pkl", "rb") as f:
    le = pickle.load(f)

# ========== Load dictionary ==========
def load_dict_from_txt(path):
    with open(path, 'r', encoding='utf-8') as f:
        lines = f.read().splitlines()
    return dict(line.split('\t') for line in lines if '\t' in line)

def load_list_from_txt(path):
    with open(path, 'r', encoding='utf-8') as f:
        return [line.strip() for line in f if line.strip()]

emoji_dict = load_dict_from_txt("emojicon.txt")
teen_dict = load_dict_from_txt("teencode.txt")
wrong_lst = load_list_from_txt("wrong-word.txt")
stopwords_lst = load_list_from_txt("vietnamese-stopwords.txt")
positive_words = load_list_from_txt("positive_VN.txt")
negative_words = load_list_from_txt("negative_VN.txt")
positive_emojis = load_list_from_txt("positive_emoji.txt")
negative_emojis = load_list_from_txt("negative_emoji.txt")

# ========== Tiá»n xá»­ lÃ½ ==========
def covert_unicode(txt):
    return txt.encode('utf-8').decode('utf-8')

def normalize_repeated_characters(text):
    return re.sub(r'(.)\1+', r'\1', text)

def process_text(text):
    document = text.lower().replace("â€™", '')
    document = regex.sub(r'\.+', ".", document)
    new_sentence = ''
    for sentence in sent_tokenize(document):
        sentence = ''.join(emoji_dict.get(c, c) for c in sentence)
        sentence = ' '.join(teen_dict.get(w, w) for w in sentence.split())
        pattern = r'(?i)\b[a-zÃ¡Ã áº£Ã£áº¡Äƒáº¯áº±áº³áºµáº·Ã¢áº¥áº§áº©áº«áº­Ã©Ã¨áº»áº½áº¹Ãªáº¿á»á»ƒá»…á»‡Ã³Ã²á»Ãµá»Ã´á»‘á»“á»•á»—á»™Æ¡á»›á»á»Ÿá»¡á»£Ã­Ã¬á»‰Ä©á»‹ÃºÃ¹á»§Å©á»¥Æ°á»©á»«á»­á»¯á»±Ã½á»³á»·á»¹á»µÄ‘]+\b'
        sentence = ' '.join(regex.findall(pattern, sentence))
        sentence = ' '.join(w for w in sentence.split() if w not in wrong_lst)
        new_sentence += sentence + '. '
    return regex.sub(r'\s+', ' ', new_sentence).strip()

def process_postag_thesea(text):
    new_document = ''
    for sentence in sent_tokenize(text):
        sentence = sentence.replace('.', '')
        lst_word_type = ['N','Np','A','AB','V','VB','VY','R']
        sentence = ' '.join(
            word[0] if word[1] in lst_word_type else ''
            for word in pos_tag(word_tokenize(sentence, format="text"))
        )
        new_document += sentence + ' '
    return regex.sub(r'\s+', ' ', new_document).strip()

def remove_stopword(text):
    return regex.sub(r'\s+', ' ', ' '.join(w for w in text.split() if w not in stopwords_lst)).strip()

def count_sentiment_items(text):
    text = str(text).lower()
    pos_word = sum(1 for word in positive_words if word in text)
    pos_emoji = sum(text.count(emoji) for emoji in positive_emojis)
    neg_word = sum(1 for word in negative_words if word in text)
    neg_emoji = sum(text.count(emoji) for emoji in negative_emojis)
    return pos_word, neg_word, pos_emoji, neg_emoji

# ========== Dá»± Ä‘oÃ¡n ==========
def predict_sentiment(text_input, recommend_num):
    text = covert_unicode(text_input)
    text = normalize_repeated_characters(text)
    text = process_text(text)
    text = process_postag_thesea(text)
    text = remove_stopword(text)

    tfidf_vector = vectorizer.transform([text])
    pos_word, neg_word, pos_emoji, neg_emoji = count_sentiment_items(text_input)
    numeric_features = scaler.transform([[pos_word, neg_word, pos_emoji, neg_emoji]])
    recommend_feature = csr_matrix([[recommend_num]])

    final_features = hstack([tfidf_vector, csr_matrix(numeric_features), recommend_feature])
    y_pred = model_lr.predict(final_features)[0]
    label = le.inverse_transform([y_pred])[0]
    return label


# ========== CÃ¡c Trang á»¨ng Dá»¥ng ==========
if menu_choice == "ğŸ“Œ Business Objective":
    st.title("ğŸ“Œ Business Objective: Sentiment Analysis and Information Clustering")
    st.markdown("""
    #### Má»¥c tiÃªu cá»§a Ä‘á»“ Ã¡n:
    
    - **Sentiment Analysis**: XÃ¢y dá»±ng mÃ´ hÃ¬nh phÃ¢n loáº¡i cáº£m xÃºc tá»« cÃ¡c Ä‘Ã¡nh giÃ¡ cá»§a nhÃ¢n viÃªn/á»©ng viÃªn vá» cÃ´ng ty trÃªn ITviec (TÃ­ch cá»±c / Trung tÃ­nh / TiÃªu cá»±c). GiÃºp cÃ´ng ty náº¯m báº¯t Ä‘Æ°á»£c tÃ¢m lÃ½ ngÆ°á»i lao Ä‘á»™ng.

    - **Information Clustering**: PhÃ¢n cá»¥m cÃ¡c Ä‘Ã¡nh giÃ¡ Ä‘á»ƒ xÃ¡c Ä‘á»‹nh Ä‘áº·c Ä‘iá»ƒm ná»•i báº­t cá»§a tá»«ng nhÃ³m cÃ´ng ty, tá»« Ä‘Ã³ Ä‘á» xuáº¥t cÃ¡c cáº£i tiáº¿n Ä‘á»ƒ giá»¯ chÃ¢n nhÃ¢n viÃªn vÃ  nÃ¢ng cao tráº£i nghiá»‡m á»©ng viÃªn.

    #### á»¨ng dá»¥ng:
    
    - Há»‡ thá»‘ng Ä‘Ã¡nh giÃ¡ ná»™i bá»™ cho cÃ¡c cÃ´ng ty
    - CÃ´ng cá»¥ gá»£i Ã½ cáº£i thiá»‡n mÃ´i trÆ°á»ng lÃ m viá»‡c
    - Tá»± Ä‘á»™ng phÃ¢n tÃ­ch hÃ ng loáº¡t Ä‘Ã¡nh giÃ¡ tá»« ná»n táº£ng tuyá»ƒn dá»¥ng
    """)

elif menu_choice == "ğŸ—ï¸ Build Model":
    st.title("ğŸ—ï¸ Build Model")
    st.write("### Sentiment Analysis")
    st.write("##### 1. Data EDA")
    st.image("Sentiment_EDA.JPG")
    st.image("Clustering_EDA.JPG")
    st.write("##### 2. Visualize")
    st.image("sentiment_distributed_data.JPG")
    st.write("##### 3. Build model and Evaluation")
    st.write("###### Chá»n 3 model Logistic Regression , Random Forest , Decision Tree")
    st.write("###### ÄÃ¡nh giÃ¡ káº¿t quáº£ dá»±a trÃªn Presicion , ReCall , F1-Score , Accuracy")
    st.image("sentiment_evaluation.JPG")
    st.write("###### Confusion Matrix")
    st.image("Confusion Matrix.JPG")
    st.markdown("Chá»n mÃ´ hÃ¬nh <span style='color: red; font-weight: bold; text-decoration: underline'>Logistic Regression</span> lÃ  tá»‘i Æ°u nháº¥t.",
    unsafe_allow_html=True)

elif menu_choice == "ğŸ’¬ Sentiment Analysis":
    st.title("ğŸ’¬ á»¨ng dá»¥ng phÃ¢n tÃ­ch cáº£m xÃºc review cÃ´ng ty")

    input_text = st.text_area("âœï¸ Nháº­p cÃ¢u Ä‘Ã¡nh giÃ¡ cá»§a báº¡n:", height=150)
    recommend_input = st.checkbox("âœ… Báº¡n cÃ³ recommend cÃ´ng ty nÃ y khÃ´ng?", value=True)
    recommend_num = 1 if recommend_input else 0

    if st.button("ğŸš€ Dá»± Ä‘oÃ¡n cáº£m xÃºc"):
        if not input_text.strip():
            st.warning("â›” Vui lÃ²ng nháº­p ná»™i dung review!")
        else:
            with st.spinner("ğŸ” Äang xá»­ lÃ½..."):
                result = predict_sentiment(input_text, recommend_num)
            st.success(f"âœ… Káº¿t quáº£ dá»± Ä‘oÃ¡n: **{result.upper()}**")

elif menu_choice == "ğŸ§© Information Clustering":
    st.title("ğŸ§© Information Clustering")
    st.info("ğŸ› ï¸ PhÃ¢n cá»¥m Ä‘Ã¡nh giÃ¡ cÃ´ng ty sáº½ Ä‘Æ°á»£c cáº­p nháº­t sau")

